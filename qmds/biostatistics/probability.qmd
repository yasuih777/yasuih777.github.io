---
image: "../../images/biostatistics.png"
title: 確率基礎
date: 01/08/2024
date-modified: 01/08/2024
abstract: 統計学の根幹である確率理論の基礎を学びます
categories:
    - 確率統計学
---

```{r}
#| label: Init R environment

# このノートで使用するライブラリです

library("tidyr")
library("ggplot2")

set.seed(777)
rm(list=ls())
```

# 確率的現象
　**確率的現象**は、とりうる選択肢の中からどれが実現するのか推測できない現象のことです。例えば、次のようなものが該当します。

- 測定ノイズ: 同じプロトコルで実験を行ってもシステムノイズや測定ノイズによって必ず同じ測定値が得られることはありません
- 遺伝, 進化: 生殖によって生まれる子は父とも母とも異なる性質を持ちます、さらに兄弟でも全く異なります
- 分子の動き: 分子は空間上をランダムウォークします、これによって化学反応系に偶発的な要素が生まれ揺らぎが発生します

# 統計解析の目的
　統計解析を行う際には、確率的現象の背後に**真の確率分布** $P_{T}$ <sup>[1](#note1)</sup> が存在し、観測データはこの真の確率分布からランダムサンプリングされたものと仮定します。この仮定が成り立たない場合には、確率論を使うことができないので平均や標準偏差を求めることもままなりません。  
　統計家は確率論を用いて測定データのモデリングや分析を行い真の確率分布がもつ性質を推定します。即ち、**統計解析は確率論を用いて測定データから真の確率分布の性質を明らかにすることを目的とします。** @fig-1

```{mermaid}
%%| label: fig-1
%%| fig-cap: 統計解析のフロー

graph LR
    Data[(測定データ)]
    Distribution([真の確率分布])
    Analyst[統計家]
    
    Distribution -.->|ランダムサンプリング| Data
    Analyst -->|観測| Data
    Analyst -->|統計解析| Distribution
```

　得られた測定データから真の確率分布の性質をどれ程正確に推定できるかは、データの質と量に大きく依存します。データにノイズが少なく量も十分であれば真の確率分布の性質を詳細に推定することができますが、一方でデータにノイズが多く量が少なければ大雑把な性質しか推定できません。  
　統計家が統計解析を行う前に考えなければならないことは、測定データがどの程度情報を含んでいるかを見極め、適切な解析手法と妥当な到達目標を設定することです。

　また、良質な測定データが得られた場合でも統計解析ではわからない、限界があることを理解しなければなりません。統計解析である現象が起こる確率が尤もらしくわかったとしても、次にその現象が起こるかどうかを確実に予測することはできません。いえるのは現象が起こるのはどれほどの頻度かというだけです。加えて、統計解析では真の確率分布を正確に推定することもできません。これまでに多くの現象が正規分布などの数式化された確率分布に近似されてきましたが、数式で表現力以上に現象は複雑であるためそれらのほとんどは厳密には間違いであるといえます。このとき議論すべきなのは、測定データからみて真の確率分布と仮定したものからどれほど誤差があるかどうかです。  
　**即ち、統計解析は仮定した真の確率分布に当てはまる部分とノイズによって揺らぐ部分に分離し、ノイズの部分は誤差を議論した上でそれ以上は追求しない手法です。**

# 確率空間

## 定義

　**確率空間**は標本空間 $\Omega$ , 事象空間 $\mathcal{E}$ , 確率測度 $P$ の三つ組 $(\Omega, \mathcal{E}, P)$ で定義されます。@fig-2

- 標本空間 $\Omega$ : 個別の確率的現象 $\omega \in \Omega$ (標本点, 根元事象)の集合です。標本空間には起こりうる全ての確率的現象が含まれており、各確率的現象は互いに排他的です。
    - ex) サイコロの目の標本空間: $\Omega = \{ 1, 2, 3, 4, 5, 6 \}$
    - 排他的でないとは、例えば例のサイコロの目の標本空間が次のように1の要素を二つ以上もつことは無いということです。
        - $\Omega = \{ 1, 1, 2, 3, 4, 5, 6 \}$ ← これは標本空間ではない
- 事象空間 $\mathcal{E}$ : 各要素 $e \in \mathcal{E}$ 標本空間の部分集合 $e \subseteq \Omega$ となる集合です。この各要素は**事象**と呼びます。事象空間には空集合 $\varnothing$ <sup>[2](#note2)</sup>と全体集合 $\Omega$ も含みます。事象空間は可算個の集合演算で閉じているため、 $\sigma$ -代数の構造を持ちます(詳細は割愛しますが、 $\sigma$ -代数であることによって集合の長さや面積などの大きさを測ることができる、即ち確率値を求めることができます)。事象空間の要素数は $|\mathcal{E}| = 2^{|\Omega|}$となります。
    - ex) サイコロの目が偶数となる事象: $e = \{ 2, 4, 6 \}$
    - サイコロの目の事象空間の要素数は $2^{|\{ 1, 2, 3, 4, 5, 6 \}|} = 2^{6} = 64$
- 確率測度 $P$ : 確率測度は写像 $P \colon \mathcal{E} \to [0, 1] \subset \mathbb{R}$ です。一つの事象 $e$ に対して**確率値** $P(e) \in [0, 1]$ が定まります。特に、$P(\varnothing) = 0$ , $P(\Omega) = 1$です。また、確率測度は各事象が互いに排反であるとき @eq-1 の(3)を満たします。
    - ex) サイコロの目が偶数となる確率: $P( \{ 2, 4, 6 \}) = \frac{1}{2}$

## Kolmogorov(コルモゴロブ)の公理
確率測度 $P$ が満たす、次の3つからなる Kolmogorovの公理 @eq-1 があります。これは前述した標本空間、事象空間、確率測度の記述と同等の意味です。

$$
\begin{split}
&(1) \ e_{1}, e_{2}, e_{3} \in \mathcal{E} \Rightarrow e_{1} \cup e_{2} \cup e_{3}, e_{1} \cap e_{2} \cap e_{3}, e_{1}^{c}, \varnothing, \Omega \in \mathcal{E} \\[1em]
&(2) \ \forall e \in \mathcal{E}, 0 \leqq P(e) \leqq 1 \ (特に P(\varnothing) = 0, \ P(\Omega) = 1) \\[1em]
&(3) \ e_{1}, e_{2} \in \mathcal{E}, e_{1} \cap e_{2} = \varnothing \ \mbox{if} \ i \ne j \Rightarrow P(\cup_{i} e_{i}) = \sum_{i} P(e_{i})
\end{split}
$${#eq-1}

　特に、実直線 $\mathbb{R}$ が標本空間であるとき、事象空間にはボレル集合族 $\mathcal{B}$ が用いられます。こちらも詳細な解説は割愛しますが、これによって任意の開区間、閉区間、点、和集合、そして積集合に対して確率を計算できます。  

# 確率変数

## 定義

**確率変数** $X$ <sup>[3](#note3)</sup> は @eq-2 の条件を満たす写像として定義されます。
$$
X \colon \Omega \rightarrow \mathbb{R}, \exists X^{-1}(b) \in \mathcal{E} \ (\mbox{for any} \ b \in \mathcal{B}) 
$${#eq-2}

　複雑になったように見えますが、この確率変数 $X$ を考えることで解析したい確率的現象にバリエーションを持たせることができます。例えば、 $b = [0, 1] \in \mathcal{B}$ をとると、 @eq-2 より $X^{-1}([0, 1]) = \{ \omega \in \Omega | 0 \leqq X(\omega) \leqq 1 \} \in \mathcal{E}$ なる $X^{-1}$ が存在し、確率値 $P(0 \leqq X \leqq 1)$ が計算できます。これは任意の区間や点で同様に確率値を求めることもできます(ex: $P(X = 0)$ )。  
　なので、確率空間 $(\Omega, \mathcal{E}, P)$ に対して確率変数 $X$ が与えられたとき、対応する確率空間 $(\Omega, \mathcal{B}, P_{X})$ が定まります。この $P_{X}$を**確率分布**といい、確率変数は確率分布 $P_{X}$ に従います( $X \thicksim P_{X}$ )。@fig-2

```{mermaid}
%%| label: fig-2
%%| fig-cap: 確率空間・確率変数のイメージ

graph LR
    Sample[標本空間]
    Borel[ボレル集合族]
    Event[事象空間]
    Probability[確率測度]

    Sample -.->|確率変数| Borel
    Borel -.->|確率変数の逆像| Event
    Event -->|写像| Probability

    subgraph 確率空間
        Sample
        Event
        Probability
    end
```

※主要な確率分布は[確率分布ノート](probability_distribution.qmd)で扱います。

## 例
例えば、サイコロを一回振って出た目を標本空間$\{ 1, 2, 3, 4, 5, 6 \} \in \Omega$として考えるとき、次のような確率変数をとることができます。

- サイコロの目をそのままとった確率変数: $X_{id} = id(\omega)$、このときの確率分布は

$$
P_{X}(X_{id} = i) = \frac{1}{6} \ (i = 1, \cdots , 6)
$$

- サイコロの目の偶数を0, 奇数を1としたときの確率分布: $X_{odd} = \omega \ mod \ 2$、このときの確率分布は

$$
P_{X}(X_{odd} = i) = \frac{1}{2} \ (i = 0, 1)
$$

- サイコロの目をコサイン変換したときの確率分布: $X_{cos} = \cos(\frac{\pi \omega}{4})$、このときの確率分布は

$$
P_{X}(X_{cos} = i) = 
\left\{
    \begin{align}
        \frac{1}{3} \ &(i = -1) \\
        \frac{1}{2} \ &(i = 0) \\
        \frac{1}{6} \ &(i = 1) 
    \end{align}
\right.
$$

# 確率密度関数・分布関数
　**確率密度関数**は $X \thicksim P_{X}$ に対して  @eq-3 を満たす関数 $f_{X}$ です。(@fig-3)

$$
P_{X}(X \in b) = \int_{b} f_{X}(x) dx \ (\mbox{for any} \ b \in \mathcal{B}
$${#eq-3}

　これは、事象 $b$ の範囲で $f_{X}$ を積分すると、事象 $b$ の確率値が求まることを表しています。

　また、確率変数 $X$ が実測値 $x$ 以下になる確率として定義される関数を**分布関数** $F_{X}$ といいます @eq-4 。(@fig-3)

$$
P_{X}(X \leqq x) = P_{X}([- \infty, x]) = F_{X}(x)
$${#eq-4}

　分布関数 $F_{X}$ は次の性質を持ちます。

- $x$ に対して広義単調増加関数
- $F_{X}(- \infty) = 0, F_{X}(\infty) = 1$
- $\forall x_{1}, x_{2} \in b, P_{X}(x_{1} \leqq X \leqq x_2) = F_{X}(x_{2}) - F_{X}(x_{1})$
- $f_{X}(x) = \frac{\partial F_{X}}{\partial x}(x)$

```{r}
#| label: fig-3
#| fig-cap: "確率密度関数と分布関数の例"
#| fig-width: 8
#| fig-height: 3

x <- seq(-5, 5, length=100)
f_x <- dnorm(x)
F_x <- pnorm(x)

data = data.frame(list(x = x, fx = f_x, Fx = F_x)) |>
    as_tibble() |>
    pivot_longer(cols = c(fx, Fx), names_to = "functions")

g <- ggplot(data = data, mapping = aes(x = x, y = value, colour = functions)) +
    geom_line(linewidth = 2, alpha = 0.75) +
    labs(x = "x", y = "y")
plot(g)
```

# 複数の確率変数の同時確率分布

## 同時確率密度関数
　**同時確率**とは、いくつかの事象 $\{ e_{1}, \cdots, e_{n} \}$ の積集合の確率 $P(e_{1}, \cdots, e_{n}) \coloneqq P(e_{1} \cap \cdots \cap e_{n})$です。  
　**同時確率密度関数** $f_{XY}$ は、確率変数 $X, Y$ に対して、 @eq-5 で定義される関数です。

$$
P((X, Y) \in I \times J) = 
\int \int_{(x, y) \in I \times J} f_{XY}(x, y) dx dy \ (\mbox{for any} \ I, J \subset \mathbb{R}^{2})
$${#eq-5}

## 周辺分布
　同時確率密度関数 $f_{XY}$ に対して、確率変数 $Y$ がとりうる全ての値の総和をとる(@eq-6)ことを**周辺化**といいます。このとき、周辺化された関数 $f_{X}$ は確率変数 $X$ の確率密度関数そのものです。

$$
\int^{\infty}_{- \infty} f_{XY}(x, y) dx = f_{X}(x)
$${#eq-6}

　これは、集団の(年齢、年収)をランダムサンプリングしたデータから年齢のデータを捨てた場合に集団の年収をランダムサンプリングしたデータと同等になります。

## 条件付き確率分布
　事象 $e_{1}, e_{2}$について、事象 $e_{2}$ が実現する条件下での事象 $e_{1}$ の**条件付き確率** $P(e_{1} | e_{2})$ は @eq-7 で定義されます。

$$
P(e_{1} | e_{2}) = \frac{P(e_{1} \cap e_{2})}{P(e_{2})}
$${#eq-7}

　特に、確率変数 $X, Y$について、 $Y=y$ のもとでの確率変数 $X$ の**条件付き確率密度関数** $f_{X | Y=y}$ を @eq-8 で定義します(一般的な記号として$p(x | y)$ と表記します)。

$$
p(x | y) = f_{X | Y=y} = \frac{f_{XY}(x, y)}{f_{Y}(y)}
$${#eq-8}

　$p(x | y)$は次のような性質を持ちます。

- $p(x | y) \geqq 0$
- $\int^{\infty}_{- \infty} p(x | y) dx = 1$

　例えば、サイコロの出た目が偶数である( $Y = y$ )という条件のもとで、出た目が4以上になる( $X$ )条件付き確率は次のように計算できます。

$$
p(x | y) = \frac{\frac{1}{6} + \frac{1}{6}}{\frac{1}{6} + \frac{1}{6} + \frac{1}{6}} = \frac{2}{3}
$$

　また、@eq-8 より、同時確率密度関数は @eq-9 のように書くことができます。

$$
f_{XY} = f_{X | Y=y} f_{Y} \ (p(x, y) = p(x | y) p(y))
$${#eq-9}

## 統計的独立
　二つの事象 $e_{1}, e_{2}$ が**統計的独立**とは、 $P(e_{1}, e_{2}) = P(e_{1}) P(e_{2})$ が成り立つことです。
　特に、確率変数 $X, Y$ について $X$ と $Y$ が統計的独立とは @eq-10 が成り立つことをいいます。

$$
\forall b_{1}, b_{2} \in \mathcal{B}, P(X \in b_{1}, Y \in b_{2}) = P(X \in b_{1}) P(Y \in b_{2})
$${#eq-10}

　確率変数 $X$ と $Y$ が統計的独立であるとき、 $p(x | y) = p(x)$ が成り立ちます。これは、確率変数 $Y$ がどんな値をとろうとも $X$ の確率分布 $P_{X}$ には影響を与えないことを意味しています。

# 期待値

## 期待値
　確率変数 $X \thicksim P_{X}$ について、その**期待値** $E(X)$は @eq-11 で定義されます。期待値は $X$ からランダムサンプリングされたデータの平均的な値を示します。

$$
E(X) = \int^{\infty}_{- \infty} x f_{X}(x) dx
$${#eq-11}

　期待値 $E(X)$は次の性質を持ちます。

- $E(\sum^{n}_{i=1} X_{i}) = \sum^{n}_{i=1} E(X_{i})$
- $a, b$ が定数のとき、$E(a X + b) = a E(X) + b$
- $X, Y$ が統計的独立のとき、 $E(X Y) = E(X) E(Y)$
- $\phi \colon X \rightarrow \mathbb{R}$ なる関数が存在するとき<sup>[4](#note4)</sup>、 $E(\phi(X)) = \int^{\infty}_{- \infty} \phi(x) f_{X}(x) dx$

　また、確率変数 $X, Y$ について、$Y = y$ のもとでの**条件付き期待値** $E(X | Y=y)$ は @eq-12 で定義されます<sup>[5](#note5)</sup>。

$$
\begin{split}
E(X | Y=y) &= \int^{\infty}_{- \infty} x f_{X}(x | y) dx \\
    &= \frac{1}{f_Y(y)} \int^{\infty}_{- \infty} x f_{XY}(x, y) dx
\end{split}
$${#eq-12}

※ @eq-8 を使用しました。

## 指示関数
　確率変数 $X$ の確率空間 $(\Omega, \mathcal{B}, P_{X})$ が与えられているとき、**指示関数** $I_{b}$ は @eq-13 で定義されます。

$$
\forall b \in \mathcal{B}, 
I_{b}(x) = 
\left\{
    \begin{align}
        1 \ \mbox{if} \ x \in b \\
        0 \ \mbox{if} \ x \notin b
    \end{align}
\right.
$${#eq-13}

　この関数は、期待値と確率を繋ぐ性質を持ちます。即ち、 $E(I_{b}(x)) = P(X \in b)$ が成り立ちます。

　そのほかにも以下の性質を持ちます。

- $I_{\mathbb{R}} = 1, I_{\varnothing} = 0$
- $(I_{b}(x))^{2} = I_{b}(x)$
- $I_{b_{1}}(x) I_{b_{2}}(x) = I_{b_{1} \cap b_{2}}(x)$
- $I_{b_{1} \cup b_{2}}(x) = I_{b_{1}}(x) + I_{b_{2}}(x) - I_{b_{1} \cap b_{2}}(x)$

# 分散・共分散

## 分散
　確率変数 $X$ の期待値を $\mu = E(X)$ とするとき、 $X$ の**分散** V(X) は @eq-14 で定義されます。分散は $X$ がランダムサンプリングするときにどれほどばらつきがあるか、即ち確率分布の広がりの尺度を与えます <sup>[6](#note6)</sup> <sup>[7](#note7)</sup>。

$$
\begin{split}
V(X) &= E((X - \mu)^{2}) \\
    &= \int^{\infty}_{- \infty} (x - \mu) f_{X}(x) dx 
\end{split}
$${#eq-14}

　分散の平方根 $\sigma = \sqrt{V(X)}$ を**標準偏差**といいます。標準偏差は期待値を単位が揃っているので加算、減算で $X$ からランダムサンプリングして得られたデータの大まかな広がりを表すことができます。

　また、分散 $V(X)$は次の性質を持ちます。

- $a, b$ が定数のとき、$V(a X + b) = a^{2} V(X)$
- $V(X) = E(X^{2}) - E(X)^{2}$
- $X, Y$ が統計的独立のとき、 $V(X + Y) = V(X) + V(Y)$

## 共分散
　確率変数 $X, Y$ の期待値をそれぞれ $\mu_{X}, \mu_{Y}$とするとき、 $X, Y$ の**共分散** $C(X, Y)$ を @eq-15 で定義します。これは確率変数間の相関の程度を表す指標で、共分散と $X = x, Y = y$ の関係性は次のようになります。

- $C(X, Y) > 0$ のとき、 $x$ が大きな値をとるときに同時に観測された $y$ も大きな値をとる傾向がある
- $C(X, Y) < 0$ のとき、 $x$ が大きな値をとるときに同時に観測された $y$ は小さな値をとる傾向がある
- $C(X, Y) = 0$ のとき、 $x$ が大きな値をとるときに同時に観測された $y$ は大きくも小さくもなる傾向がない

$$
\begin{split}
C(X, Y) &= E(XY) - E(X)E(Y) \\
    &= E((X - \mu_{X})(Y - \mu_{Y}))
\end{split}
$${#eq-15}

　また、共分散 $C(X, Y)$ は次の性質を持ちます。

- $C(X, X) = V(X)$
- $a, b, c, d$ が定数のとき、$C(a X + b, c Y + d) = a c C(X, y)$
- $X, Y$ が統計的独立ならば $C(X, Y) = 0$ (また逆は偽であることに注意)
- $V(X + Y) = V(X) + V(Y) + 2 C(X, Y)$

　確率変数を要素に持つベクトル $\mathbb{X} = (X_{1}, \cdots, X_{m})^{T}$ に対して、第 $(i, j)$ 成分が $C(X_{i}, X_{y})$ で与えられる $(m \times m)$ 行列 $\mathbb{V}(\mathbb{X})$ を**分散共分散行列**を呼びます(ここで、 $\mu = E(\mathbb{X})$ です)。この分散共分散行列は半正定値実対象行列です。

$$
\begin{split}
\mathbb{V}(\mathbb{X}) = E((\mathbb{X} - \mu)(\mathbb{X} - \mu)^{T}) \\
[\mathbb{V}(\mathbb(X))]_{ij} = C(X_{i}, X_{y})
\end{split}
$$

# 相関係数
　確率変数 $X, Y$が $V(X) > 0, V(Y) > 0$を満たすとき、**相関係数** $\rho(X, Y)$ @eq-16 で定義されます <sup>[8](#note8)</sup>。相関係数は二変数間の線形的な関係の尺度を与えます @tbl-1。

$$
\rho(X, Y) = \frac{C(X, Y)}{\sqrt{V(X)V(Y)}}
$${#eq-16}

　相関係数は次の性質を持ちます。

- $-1 \leqq \rho(X, Y) \leqq 1$
- 特に、 $\rho(X, Y) = \pm 1$のとき、 $\frac{X - E(X)}{\sqrt{V(X)}} \mp \frac{Y - E(Y)}{\sqrt{V(Y)}} = 0$ (複合同順)

　また、相関係数による尺度は明確に決まっていませんが、次の基準をよく見ます。

| 相関係数 | 解釈 |
| :---: | --- |
| $-1.0 \leqq \rho \leqq -0.7$ | 負の強相関 |
| $-0.7 \leqq \rho \leqq -0.4$ | 負の相関 |
| $-0.4 \leqq \rho \leqq -0.2$ | 負の弱相関 |
| $-0.2 \leqq \rho \leqq 0.2$ | 無相関 |
| $0.2 \leqq \rho \leqq 0.4$ | 正の弱相関 |
| $0.4 \leqq \rho \leqq 0.7$ | 正の相関 |
| $0.7 \leqq \rho \leqq 1.0$ | 正の強相関 |

: 相関係数が与える解釈 {#tbl-1}

# 標本からの推定値

## 推定値
　$X \thicksim P_{X}$なる確率変数からランダムサンプリングして得られた標本の集合 $\{ x_{i} | i = 1, \cdots, n \}$ に対して、確率分布の期待値の推定値である**標本平均** $\bar{x}$ @eq-17 と分散の推定値である**標本分散** $s^{2}$ @eq-18 が定義されます。また、 $s = \sqrt{s^{2}}$ を**標本標準偏差**と定義します。

$$
\bar{x} = \frac{1}{n} \sum^{n}_{i = 1} x_{i}
$${#eq-17}

$$
s^{2} = \frac{1}{n - 1} \sum^{n}_{i = 1} (x_{i} - \bar{x})^{2}
$${#eq-18}

　加えて、 $(X, Y) ~ P$ なる確率変数からランダムサンプリングして得られたペアの標本の集合 $\{ (x_{i}, y_{i}) | i = 1, \cdots, n \}$ に対して、共分散の推定値である**標本共分散** $c_{XY}$ @eq-19 と相関係数の推定値である**標本相関係数** $r_{XY}$ @eq-20 が定義されます。

$$
c_{XY} = \frac{1}{n-1} \sum^{n}_{i = 1} (x_{i} - \bar{x}) (y_{i} - \bar{y})
$${#eq-19}

$$
r_{XY} = \frac{c_{XY}}{s_{X} s_{Y}}
$${#eq-20}

## 点推定・標本変量
　真の確率分布が既知であるが、その分布のあるパラメータ(母数) $\theta$ が未知であるとき、標本から適当な値 $\hat{\theta}_{n}(x_{1}, \cdots, x_{n})$ を計算し、この値を $\theta$ の推定値とする。**点推定法**があります。例えば、母平均 $\mu$ を標本平均 $\hat{\mu} \equiv \bar{x}$ @eq-16 で推定するのは母数 $\mu$ の $\bar{x}$ による点推定といえます。@fig-4

```{mermaid}
%%| label: fig-4
%%| fig-cap: 点推定法の流れ

graph LR
    Population([真の確率分布])
    Sample[(標本)]
    Parameter[母数]

    Population ==>|抽出| Sample
    Sample -->|推定| Parameter
```

　ここで、実際に得られる推定値 $\hat{\theta}_{n}$ は必ず母数 $\theta$ と一致することはなく、ある程度のばらつきをもって分布している<u>ある変量</u>の一標本として得られたものであることに注意が必要です @fig-5 。このある変量がどんな分布に従っているかを知ることは推定値を位置づけるうえでとても重要です。

```{r}
#| label: fig-5
#| fig-cap: "真の確率分布(標準正規分布)の母平均=0: 黒線、および標本平均: ヒストグラムの関係性。標本平均は母平均付近には局在しているが、全て黒線と一致しない。"
#| fig-width: 8
#| fig-height: 3

n <- 50
iter <- 1000

sample <- matrix(rnorm(n * iter), nrow=iter)
data = data.frame(sample=rowMeans(sample)) |>
    as_tibble()

g <- ggplot(data = data, mapping = aes(x = sample)) +
    geom_histogram(
        aes(y=..density..), binwidth = 0.05, color="white", alpha=0.5
    ) +
    geom_vline(xintercept=0, size=2) +
    scale_x_continuous(limits = c(-1, 1)) +
    labs(x = "Sample mean", y = "Density")

plot(g)
```

　そこで、標準の集団 $\{ x_{i} | i = 1, \cdots n \}$ を実測値とする、真の確率分布に従い互いに統計的独立な確率変数の組 $(X_{1}, \cdots, X_{n})$ を考えます。これは**標本変量**と呼びます。この標本変量から作られる確率変数 $\bar{X}$ @eq-21 を**標本変量平均**といいます。標本平均 $\bar{x}$ は標本変量平均の実測値となります。同様に標本変量から**標本変量分散(不偏分散)** $S^{2}$ @eq-22 を定義することができます。

$$
\bar{X} = \frac{X_{1} + \cdots + X_{n}}{n}
$${#eq-21}

$$
S^{2} = \frac{1}{n - 1} \sum^{n}_{i = 1} (X_{i} - \bar{X})^{2}
$${#eq-22}

　標本変量平均 $\bar{X}$ と不偏分散 $S^{2}$ は以下の性質をもちます(ただし、母平均を $\mu$, 母分散を $\sigma^{2}$ としました)。ここで重要なのは、標本変量平均および不変分散の期待値が母平均および母分散と一致することです。これは、推定値の不偏性、即ち推定値が母平均および母分散を表現しうることを表しています。

- $E(\bar{X}) = \mu$
- $V(\bar{X}) = \frac{\sigma^{2}}{n}$
- $E(S^{2}) = \sigma^{2}$

　上記のように標本変量を導入することでさまざまな母数 $\theta$ の推定値 $\hat{\theta}_{n}$ を評価することができます。推定値がきちんと真の確率分布の母数を表現しうるならば、母数の周りに分布していることが望まれます。このような推定値 $\hat{\theta}_{n}$ は @eq-23 が成り立ち、**不偏推定量**と呼ばれます <sup>[9](#note9)</sup>。

$$
E(\hat{\theta}_{n}) = \theta
$${#eq-23}

　留意すべきなのは、全ての母数に対して不偏推定量が存在するわけではないということです。例えば、上記の標本平均、標本分散(標本標準偏差)、標本共分散は不偏推定量ですが、標本相関係数は一般には不偏推定量ではないことが明らかになっています。

# 特性関数
　確率変数 $X$ の確率分布 $p(x)$ を完全に特徴づける関数として、 $X$ の**特性関数** $\phi_{X}(t)$ @eq-24 が定義されます。

$$
\phi_{X}(t) = E(e^{itX})
$${#eq-24}

　特性関数では「確率変数 $X$ と確率変数の列 $X_{n} \ (n = 1, 2, \cdots)$ が、 $\forall t \in \mathbb{R}, \lim_{n \rightarrow \infty} \phi_{X_{n}}(t) = \phi_{X}(t)$ を満たし、 $\phi_{X}(t)$ が $t = 0$ で連続であれば $X_{n}$ は $X$ に分布収束する」という**レヴィの連続性定理** <sup>[10](#note10)</sup>を持ちます。これは、大雑把にいうと「特性関数が一致する確率変数は同じ確率分布を持つ」ことを意味し、特性関数を比較することによって二つの確率分布の等価性を評価することができます。

　また、 $E(x^{m}) \ (m = 0, 1, 2, \cdots)$ を $X$ の **$m$ 次モーメント** といいます。このモーメント情報が全てあれば、確率分布を完全に特徴づけられます。例えば、平均や分散、歪度、尖度をモーメントから求めることができます。

　加えて、特性関数は以下の性質を持ちます($a, b, c$は定数とする)。

- $\phi_{a X + b Y}(t) = \phi_{X}(a t) \phi_{Y}(b t)$
- $\phi_{a X + c}(t) = \phi_{X}(a t) e^{itc}$

# カルバック・ライブラー情報量(KL-divergence)

　機械学習の理論で、二つの確率分布 $p(x), q(x)$ の乖離度を表す指標として、**カルバック・ライブラー情報量(KL-divergence)** $KL(p, q)$ @eq-25 がよく使われます。

$$
KL(p, q) = \int^{\infty}_{- \infty} p(x) \log \bigg( \frac{p(x)}{q(x)} \bigg) dx
$${#eq-25}

　KL-divergenceは非負で、$p = q$ のときのみ $0$ となります。また、距離ではない <sup>[11](#note11)</sup> ダイバージェンスとして定義されています。加えて、最尤推定法やベイズ統計でもパラメータを最適化する上でよく出てくる優れた指標です。

# 注釈
1. <small id="note1">母集団分布と同等です。</small>  
1. <small id="note2">このとき零事象となります。</small>
1. <small id="note3">離散か連続かで記号や名前が変わることがありますが、ここでは連続の方で書いていきます(離散の場合もおおよその考え方は同じです)。</small>
1. <small id="note4">一般的にはBaire関数をとります。</small>
1. <small id="note5">これを $y$ の関数とみて、 $X$ の $Y$ に対する回帰関数と呼ぶことがあります。</small> 
1. <small id="note6">$V(X) = 0$ となる場合はランダムサンプリングするたびに $x = \mu$ が得られます。このような確率変数は[退化分布](probability_distribution.qmd#退化分布)に従い( $X \thicksim Deg(\mu)$ )、定数確率変数と呼びます。</small>
1. <small id="note7">平均、分散から導かれる確率論における重要な性質としてチェビシェフの不等式があります。</small>
1. <small id="note8">ここの定義はピアソンの相関係数です。</small>
1. <small id="note9">不偏性に加えて最適な推定量の条件として、有効性・充足性・一致性があります。</small>
1. <small id="note10">中心極限定理や大数の法則の証明で使います。</small>
1. <small id="note10">距離の公理の内、対称性と三角不等式を満たしません。</small>

::: {#refs}
:::
